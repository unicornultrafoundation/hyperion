# Hyperion

Hyperion is a system testing infrastructure for Sonic blockchain networks. It provides tools for running network scenarios, load testing, and monitoring blockchain performance.

## Prerequisites

- Docker: https://docs.docker.com/get-docker/
- go 1.24+: https://golang.org/doc/install
- solc: https://docs.soliditylang.org/en/v0.8.13/installing-solidity.html
- go-ethereum's abigen:
  - Checkout [go-ethereum](https://github.com/ethereum/go-ethereum/) `git clone https://github.com/ethereum/go-ethereum/`
  - Checkout the right version `git checkout v1.10.8`
  - Build Geth will all tools: `cd go-ethereum` and `make all`
  - Copy `abigen` from `build/bin/abigen` into your PATH, e.g.: `cp build/bin/abigen /usr/local/bin`

## Building

To build the project, run

```
make -j
```

This will build the required docker images (make sure you have Docker access permissions!) and the Hyperion go application. To run tests, use

```
make test
```

To clean up a build, use `make clean`.

## Running

To run Hyperion, you can run the `hyperion` executable created by the build process:

```
build/hyperion <cmd> <args...>
```

To list the available commands, run

```
build/hyperion
```

# Developer Information

## Using Docker

Some experiments simulate network using Docker. For a local development the Docker must be installed:

- MacOS: https://docs.docker.com/desktop/install/mac-install/
- Linux: https://docs.docker.com/engine/install/ubuntu/

### Permissions on Linux

After installation, make sure your user has the needed permissions to run docker containers on your system. You can test this by running

```
docker images
```

If you get an error stating a lack of permissions, you might have to add your non-root user to the docker group (see [this stackoverflow post](https://stackoverflow.com/questions/48957195/how-to-fix-docker-got-permission-denied-issue) for details):

```
sudo groupadd docker
sudo usermod -aG docker $USER
newgrp docker
```

If the `newgrp docker` command is not working, a `reboot` might help.

### Docker Sock on MacOS

If Hyperion tests produce error that Docker is not listening on `unix:///var/run/docker.sock`, execute

- `docker context inspect` and make note of `Host`, which should be `unix:///$HOME/.docker/run/docker.sock`
- export system variable, i.e. add to either `/etc/zprofile` or `$HOME/.zprofile`:
  ```
  export DOCKER_HOST=unix://$HOME/.docker/run/docker.sock
  ```

## Scenarios

Scenarios are defined in YAML files and describe the network topology, applications, and test parameters. Examples can be found in the `scenarios/` directory.

## External Chain Support

Hyperion can connect to existing blockchain networks instead of creating new Docker containers. Use the `--external-rpc` flag to connect to your local chain:

```bash
build/hyperion run scenarios/external_chain.yml --external-rpc http://localhost:18545 --external-chain-id 4002
```

# Analyzing Build-In Metrics

Hyperion manages and observes a network of Opera nodes and collects a set of metrics. The metrics are automatically enabled and their outcome is stored in a CSV file, which allows for later processing in spreadsheet software.

For instance, metric data can be generated by just running the example scenario:

```
build/hyperion run scenarios/small.yml
```

which produces a directory filled with measurment results, which is printed at the end of the application output. Look for two lines like

```
Monitoring data was written to /tmp/hyperion_data_<random_number>
Raw data was exported to /tmp/hyperion_data_<random_number>/measurements.csv
```

The first line lists the directory in which all monitoring data was written to. This, in particular, includes the `measurements.csv` file, containing most of the collected monitoring data in a CSV format. It merges all the metrics in one file, and every line is one result of a single meassurement. The header of the file is:

```
| Metric | Network | Node | App | Time | Block | Workers | Value |
```

- Metric -- is the string name of the metric
- Network -- is the network name, currently always the same
- Node -- if the metric is attached to a node, the name is shows, otherwise the column is empty
- App -- if the metric is attached to an application (smart contract), the name is shows, otherwise the column is empty
- Time -- if the metric is meassured for time series (i.e. time on X-axis), the timestamp is provided, otherwise the column is empty
- Block -- if the metric is meassured for block series (i.e. block height on X-axis), the block number is provided, otherwise the column is empty
- Workers -- if the metric is meassured for the number of workers sending transactions (i.e. the number of workers on X-axis), the number is provided, otherwise the column is empty
- Value -- this column is always filled and contains the actual valu (i.e. Y-axis) meassured for the metrcis.

It means that Metrics can meassure values for block numbers or timeseries, and it can be done for the whole network, individual nodes, or applications. The metrics are all stored in the same file and values
that do not apply for particular metric are left empty.

This structure allows for easily filtering metrics of interest and importing them in a unified format to a spreadshead. The rows oriented format can be turned into rows/cells format using a Pivot table.

For instance, lets analyse the transaction throughput of the nodes. List the metric using grep:

```
grep TransactionsThroughput output.csv
```

or directly store the result to the clipboard (MacOS)

```
grep TransactionsThroughput output.csv | pbcopy
```

The content of the clipboard can be inserted into Google Sheet. For the Pivot table to work, the header must be in the first row.
When the rows are inserted, it must be clicked to `Split text column`, then the data is ready:

<img width="875" alt="image" src="https://github.com/Fantom-foundation/Norma/assets/7114574/29b51cf6-8d9e-44d6-a1e1-d8984902451e">

Notice that the CSV file could be inserted as whole (`cat output.csv | pbcopy`) to have all metrics at hand for the analysis.
This can be impossible for same large files though, as for instance a spreadsheet tool can become unresponsive.

To create the Pivot table, one has to click: `Insert -> Pivot table`, Select `data range` and Insert to a `New sheet`

<img width="853" alt="image" src="https://github.com/Fantom-foundation/Norma/assets/7114574/c3b8086b-37bb-49c8-bfdc-edff68a35ddb">
<img width="851" alt="image" src="https://github.com/Fantom-foundation/Norma/assets/7114574/9cc049b0-f919-44b2-bdd6-f5f5599b7cf7">

The new empty Pivot table will pop-up. What to show in the table depends on particular needs, but since the metric we have chosen contains the throughput
of each node, meassured for block height, it is a good idea to have the nodes as columns and values as rows, the first row being the block number. It must be set:

- Rows: `Block`
- Column: `Node`
- Values: `Value`

Notice that the items selected from drop down menus are actually the columns from the flat CSV file that has been imported.

The metrics used actually contains three additional metricts, in total:

- `TransactionsThroughput` - transaction throughput for every block and node
- `TransactionsThroughputSMA_10` - simple moving average for 10 blocks
- `TransactionsThroughputSMA_100` - simple moving average for 100 blocks
- `TransactionsThroughputSMA_1000` - simple moving average for 1000 blocks

To see only metric of interest, one has to filter it in the `Filters` drop down menu.

Notice that the Pivot table groups potentially clashing rows (like SQL GROUP BY) and applies a selected function such as Sum, Avr, Max, Min etc. At the moment we do not have metrics where such a grouping would make sense, i.e. it is imporrtant to enable filter just for one metric at a time, and then the applied grouping can be ignored (it cannot be dissabled).
Also it is good to uncheck `Show totals` for many metrics where the sums do not make sense.

As a last step, charts can be plot from the data as usual, like this:

<img width="2413" alt="image" src="https://github.com/Fantom-foundation/Norma/assets/7114574/5aeba3ec-a7ea-4b67-9aa4-12453bd149e6">

## CPU Profile Data

In addition to the Hyperion metrics, the `pprof` CPU proifile is collected every 10s from each node. The profiles are stored in the temp directory. The directory name is printed together with the Hyperion output, for instance:

```
Monitoring data was written to /tmp/hyperion_data_1852477583
```

The directory has the following structure:

```
/tmp/hyperion_data_<rand>
+ - cpu_profiles
  + - <node-name>
    + - <sample_number>.prof
    | - <sample_number>.prof
      ...
```

These files can be transfered to a developer's machine, and analysed by running

```
go tool pprof -http=":8000" <sample_number>.prof
```

## Known Hyperion Restrictions

Known restrictions

- only one node will be a validator, and it is the first node to be started; this node must life until the end of the scenario
- currently, all transactions are send to the validator node
